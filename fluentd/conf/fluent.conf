# FluentD Configuration for DDoS Detection System
# This configuration centralizes logging from all components

# Global settings
<system>
  log_level info
  suppress_repeated_stacktrace true
  emit_error_log_interval 60
  suppress_config_dump true
</system>

# Input sources - collect logs from various components
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Input for system logs
<source>
  @type tail
  path /fluentd/logs/*.log
  pos_file /fluentd/logs/fluentd-tail.log.pos
  tag ddos.system
  format json
  read_from_head true
</source>

# Input for application logs via HTTP
<source>
  @type http
  port 9880
  bind 0.0.0.0
  tag ddos.app
</source>

# Parse and enrich logs
<filter ddos.**>
  @type parser
  key_name log
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
</filter>

# Add metadata to all logs
<filter ddos.**>
  @type record_transformer
  <record>
    environment production
    service ddos_detection
    hostname ${hostname}
  </record>
</filter>

# Route logs based on type
<match ddos.producer>
  @type copy
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix ddos-producer
    include_tag_key true
    tag_key @log_name
    flush_interval 1s
    retry_max_interval 30
    retry_forever false
    <buffer>
      @type memory
      flush_interval 1s
      chunk_limit_size 2M
      queue_limit_length 8
      retry_max_interval 30
      retry_forever false
    </buffer>
  </store>
  <store>
    @type kafka2
    brokers kafka:29092
    topic ddos-logs
    compression_codec gzip
    max_send_retries 5
    required_acks -1
    <format>
      @type json
    </format>
    <buffer>
      @type memory
      flush_interval 1s
      chunk_limit_size 2M
      queue_limit_length 8
      retry_max_interval 30
      retry_forever false
    </buffer>
  </store>
</match>

<match ddos.consumer>
  @type copy
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix ddos-consumer
    include_tag_key true
    tag_key @log_name
    flush_interval 1s
    retry_max_interval 30
    retry_forever false
    <buffer>
      @type memory
      flush_interval 1s
      chunk_limit_size 2M
      queue_limit_length 8
      retry_max_interval 30
      retry_forever false
    </buffer>
  </store>
  <store>
    @type kafka2
    brokers kafka:29092
    topic ddos-logs
    compression_codec gzip
    max_send_retries 5
    required_acks -1
    <format>
      @type json
    </format>
    <buffer>
      @type memory
      flush_interval 1s
      chunk_limit_size 2M
      queue_limit_length 8
      retry_max_interval 30
      retry_forever false
    </buffer>
  </store>
</match>

<match ddos.ml_api>
  @type copy
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix ddos-ml-api
    include_tag_key true
    tag_key @log_name
    flush_interval 1s
    retry_max_interval 30
    retry_forever false
    <buffer>
      @type memory
      flush_interval 1s
      chunk_limit_size 2M
      queue_limit_length 8
      retry_max_interval 30
      retry_forever false
    </buffer>
  </store>
  <store>
    @type kafka2
    brokers kafka:29092
    topic ddos-logs
    compression_codec gzip
    max_send_retries 5
    required_acks -1
    <format>
      @type json
    </format>
    <buffer>
      @type memory
      flush_interval 1s
      chunk_limit_size 2M
      queue_limit_length 8
      retry_max_interval 30
      retry_forever false
    </buffer>
  </store>
</match>

<match ddos.predictions>
  @type copy
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix ddos-predictions
    include_tag_key true
    tag_key @log_name
    flush_interval 1s
    retry_max_interval 30
    retry_forever false
    <buffer>
      @type memory
      flush_interval 1s
      chunk_limit_size 2M
      queue_limit_length 8
      retry_max_interval 30
      retry_forever false
    </buffer>
  </store>
  <store>
    @type kafka2
    brokers kafka:29092
    topic ddos-predictions
    compression_codec gzip
    max_send_retries 5
    required_acks -1
    <format>
      @type json
    </format>
    <buffer>
      @type memory
      flush_interval 1s
      chunk_limit_size 2M
      queue_limit_length 8
      retry_max_interval 30
      retry_forever false
    </buffer>
  </store>
</match>

<match ddos.system>
  @type copy
  <store>
    @type elasticsearch
    host elasticsearch
    port 9200
    logstash_format true
    logstash_prefix ddos-system
    include_tag_key true
    tag_key @log_name
    flush_interval 1s
    retry_max_interval 30
    retry_forever false
    <buffer>
      @type memory
      flush_interval 1s
      chunk_limit_size 2M
      queue_limit_length 8
      retry_max_interval 30
      retry_forever false
    </buffer>
  </store>
  <store>
    @type kafka2
    brokers kafka:29092
    topic ddos-system-logs
    compression_codec gzip
    max_send_retries 5
    required_acks -1
    <format>
      @type json
    </format>
    <buffer>
      @type memory
      flush_interval 1s
      chunk_limit_size 2M
      queue_limit_length 8
      retry_max_interval 30
      retry_forever false
    </buffer>
  </store>
</match>

# Catch-all for any unmatched logs
<match ddos.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix ddos-general
  include_tag_key true
  tag_key @log_name
  flush_interval 1s
  retry_max_interval 30
  retry_forever false
  <buffer>
    @type memory
    flush_interval 1s
    chunk_limit_size 2M
    queue_limit_length 8
    retry_max_interval 30
    retry_forever false
  </buffer>
</match> 